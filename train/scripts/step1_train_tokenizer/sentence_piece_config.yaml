input: /home/ucllm_nedo_prod/data_management/output/merged/merged.jsonl
#input: /persistentshare/storage/team_nakamura/yamaguchi/data/text/0313wiki_1000000.jsonl
#input: /persistentshare/storage/team_nakamura/member/yamaguchi/data/text/0313wiki.jsonl
#output_dir: /persistentshare/storage/team_nakamura/member/yamaguchi/ext_hrk_ymgch_gmail_com/ucllm_nedo_prod/train/scripts/step1_train_tokenizer/output_model/wiki_65k_vocab_1000000
output_dir: /home/ucllm_nedo_prod/train/scripts/step1_train_tokenizer/output_model/wiki_65k_vocab/
#output_dir: /persistentshare/storage/team_nakamura/member/yamaguchi/ext_hrk_ymgch_gmail_com/ucllm_nedo_prod/train/scripts/step1_train_tokenizer/wiki_65k_vocab
vocab_size: 65000
num_threads: 32
model_prefix: tokenizer
character_coverage: 0.9995
model_type: unigram #character_coverage
train_extremely_large_corpus: True